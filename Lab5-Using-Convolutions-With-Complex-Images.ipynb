{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Epi-5-Lab5-Using-Convolutions-With-Complex-Images.ipynb-JP","provenance":[{"file_id":"https://github.com/lmoroney/mlday-tokyo/blob/master/Lab5-Using-Convolutions-With-Complex-Images.ipynb","timestamp":1589585527212}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wgkyeeFSVr0Y","colab_type":"code","colab":{}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"on_SDWrtZEld","colab_type":"text"},"source":["##Using Convolutions with Complex Images\n","\n","前回は、Fashion MNISTデータセットを使って画像分類器をトレーニングしました。この場合、分類するものが28x28の画像の中央にありました。これを次のレベルへと発展させ、分類するものが画像のどこにでもあっても画像の特徴を認識するための訓練を行います。\n","\n","与えられた画像に馬もしくは人間が含まれているかを分類する分類器を構築します。それを実現するためにネットワークを訓練します。\n"]},{"cell_type":"markdown","metadata":{"id":"fbJ4UFeAZd6i","colab_type":"text"},"source":["Fashion MNISTの場合、データはKeras経由でTensorFlowに組み込みました。今回はそのようなデータがないので、訓練を行う前にデータの処理を行う必要があります。\n","\n","まずはデータをダウンロードしてみましょう。\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RXZT2UsyIVe_","colab":{}},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n","    -O /tmp/horse-or-human.zip\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n","    -O /tmp/validation-horse-or-human.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9brUxyTpYZHy","colab_type":"text"},"source":["以下のpythonコードはpythonのOSライブラリを使用して、実行環境にあるファイルシステムへのアクセス権を与えます。そうすることで、zipfileライブラリを使用してファイルシステムに保存しているデータを解凍することができます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PLy3pthUS0D2","colab":{}},"source":["import os\n","import zipfile\n","\n","local_zip = '/tmp/horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/horse-or-human')\n","local_zip = '/tmp/validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/validation-horse-or-human')\n","zip_ref.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o-qUPyfO7Qr8"},"source":["zipファイルの内容はベースディレクトリ `/tmp/horse-or-human` に展開され、 `horses` と `humans` というディレクトリが含まれています。\n","\n","トレーニングセットは、ニューラルネットワークに「馬はこう見えます」「人間はこう見える」ということを教えるために使用されるデータです。\n","\n","このサンプルで注意すべきことが一つあります。このサンプルでは、画像に馬か人間かを明示的にラベル付けされていません。先ほどのファッションの例では、「これは1です」「これは7です」などとラベル付けされていました。\n","\n","*ImageGenerator* と呼ばれるものが使用されます。これは、サブディレクトリから画像を読み込んで、そのサブディレクトリの名前から自動的にラベルを付けるようにコード化されています。例えば、'training' ディレクトリに 'horses' ディレクトリと 'human' ディレクトリがあるとします。ImageGenerator が画像に適切なラベルを付けてくれるので、コーディングの手間が省けます。\n","\n","それでは、それぞれのディレクトリを定義してみましょう。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NR_M9nWN-K8B","colab":{}},"source":["# Directory with our training horse pictures\n","train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n","\n","# Directory with our training horse pictures\n","validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LuBYtA_Zd8_T"},"source":["では、`horses` と `humans` のトレーニングディレクトリのファイル名がどのようになっているか見てみよう。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4PIP1rkmeAYS","colab":{}},"source":["train_horse_names = os.listdir(train_horse_dir)\n","print(train_horse_names[:10])\n","\n","train_human_names = os.listdir(train_human_dir)\n","print(train_human_names[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HlqN5KbafhLI"},"source":["馬と人のそれぞれの画像の数を調べてみましょう。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H4XHh2xSfgie","colab":{}},"source":["print('total training horse images:', len(os.listdir(train_horse_dir)))\n","print('total training human images:', len(os.listdir(train_human_dir)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C3WZABE9eX-8"},"source":["では、いくつかの写真を見てみましょう。まず、matplotのパラメータを設定します。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b2_Q0-_5UAv-","colab":{}},"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Parameters for our graph; we'll output images in a 4x4 configuration\n","nrows = 4\n","ncols = 4\n","\n","# Index for iterating over images\n","pic_index = 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xTvHzGCxXkqp"},"source":["8頭の馬と8人の人間の写真を表示します。セルを再実行すると、毎回新しい画像が表示されます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Wpr8GxjOU8in","colab":{}},"source":["# Set up matplotlib fig, and size it to fit 4x4 pics\n","fig = plt.gcf()\n","fig.set_size_inches(ncols * 4, nrows * 4)\n","\n","pic_index += 8\n","next_horse_pix = [os.path.join(train_horse_dir, fname) \n","                for fname in train_horse_names[pic_index-8:pic_index]]\n","next_human_pix = [os.path.join(train_human_dir, fname) \n","                for fname in train_human_names[pic_index-8:pic_index]]\n","\n","for i, img_path in enumerate(next_horse_pix+next_human_pix):\n","  # Set up subplot; subplot indices start at 1\n","  sp = plt.subplot(nrows, ncols, i + 1)\n","  sp.axis('Off') # Don't show axes (or gridlines)\n","\n","  img = mpimg.imread(img_path)\n","  plt.imshow(img)\n","\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5oqBkNBJmtUv"},"source":["## Building a Small Model from Scratch\n","\n","続ける前にモデルの定義を始めましょう。\n","\n","Step 1では、tensorflowをインポートします。"]},{"cell_type":"code","metadata":{"id":"qvfZg3LQbD-5","colab_type":"code","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  import tensorflow as tf\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3lh47YWWL3x","colab_type":"code","colab":{}},"source":["print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BnhYCP4tdqjC"},"source":["次に、先ほどの例と同様に畳み込みレイヤーを追加し、最終的な結果を平坦化して全結合層にフィードします。"]},{"cell_type":"markdown","metadata":{"id":"gokG5HKpdtzm","colab_type":"text"},"source":["最後に、全結合層を追加します。\n","\n","我々は2クラス分類問題、すなわち*二項分類問題*に直面しているので、我々のネットワークは[*sigmoid*](https://wikipedia.org/wiki/Sigmoid_function)で終了することに注意してください。これにより、ネットワークの出力は0から1の間の1つのスカラー値となり、現在の画像がクラス1である確率を出力します（クラス0ではなく）．"]},{"cell_type":"code","metadata":{"id":"PixZ2s5QbYQ3","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', \n","                           input_shape=(300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s9EaFDP5srBa"},"source":["model.summary()メソッドを呼び出すと、ニューラルネットワークのサマリーが表示されます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7ZKj8392nbgP","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DmtkTn06pKxF"},"source":["\"output shape\"の列は、各レイヤーで特徴マップのサイズがどのように変化するかを示しています。畳み込みレイヤーはパディングのためにサイズを少し小さくし、各プーリングレイヤーは寸法を半分にしています。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PEkKSpZlvJXA"},"source":["次に、モデル学習のための仕様を設定します。これはバイナリ分類問題であり、最終的な活性化関数はシグモイドであるため、`binary_crossentropy`の損失でモデルを訓練します。学習率が `0.001` の `rmsprop` オプティマイザを使用します。学習中は、分類の精度を監視したいと思います。\n","\n","**注**. この場合、[確率的勾配降下](https://developers.google.com/machine-learning/glossary/#SGD) (SGD)よりも[RMSprop最適化アルゴリズム](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp)を使用する方が、RMSpropが学習レートのチューニングを自動化してくれるので好ましいです（それ以外のオプティマイザ [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) や [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad) などのオプティマイザも、学習中に学習率を自動的に調整してくれるので、ここでも同様に機能します)。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8DHWhFP_uhq3","colab":{}},"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=0.001),\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sn9m9D3UimHM"},"source":["### Data Preprocessing\n","\n","ソースフォルダ内の画像を読み込んで `float32` のテンソルに変換し、（ラベルを付けて）ネットワークに送り込むデータ生成器をセットアップしましょう。学習用、テスト用にそれぞれ1つずつの生成器を用意します。生成器は、サイズ300x300の画像とそのラベル（バイナリ）のバッチを生成します。\n","\n","すでにご存知かもしれませんが、ニューラルネットワークに入力されるデータは、通常、ネットワークで処理しやすいように何らかの方法で正規化されているはずです(生のピクセル・データをニューラルネットワークにそのまま投入するのは珍しいことです)。ここでは、ピクセル値を `[0, 1]` の範囲になるように正規化することで画像を前処理します(元のピクセルデータはすべての値が `[0, 255]` の範囲になります)。\n","\n","Kerasでは、`keras.preprocessing.image.ImageDataGenerator`クラスで `rescale` パラメータを使用してこれを行います。この `ImageDataGenerator` クラスを使うと、`.flow(data, labels)` または `.flow_from_directory(directory)` を使って拡張画像バッチ（とそのラベル）のジェネレータをインスタンス化することができます。これらのジェネレータは、データジェネレータを入力として受け取るKerasモデルメソッド（`fit_generator`, `evaluate_generator`, `predict_generator`が該当します）で利用できます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ClebU9NJg99G","colab":{}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        '/tmp/horse-or-human/',  # This is the source directory for training images\n","        target_size=(300, 300),  # All images will be resized to 150x150\n","        batch_size=128,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n","        target_size=(300, 300),  # All images will be resized to 150x150\n","        batch_size=32,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mu3Jdwkjwax4"},"source":["### Training\n","\n","15エポックの訓練をしてみましょう -- これは実行に数分かかるかもしれません。\n","\n","エポックごとの値に注意してください（小林注．それぞれの指標の上下動に注目してくださいという意味です）。\n","\n","損失と精度は訓練の進捗状況を示す大きな指標です。これは訓練データの分類を推測し、既知のラベルと比較して結果を計算します。精度とは、正しく推測できたデータの割合を意味します。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Fb1_lgobv81m","colab":{}},"source":["history = model.fit(\n","      train_generator,\n","      validation_data = validation_generator,  \n","      epochs=15,\n","      steps_per_epoch=8,\n","      validation_steps=8,\n","      verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6vSHzPR2ghH","colab_type":"text"},"source":["###Running the Model\n","\n","それでは、実際にモデルを使って予測を実行してみましょう。このコードでは、ファイルシステムから1つ以上のファイルを選択し、それらをアップロードして、モデルを通して実行し、オブジェクトが馬か人間かを表示します。\n","\n","インターネットからファイルシステムに画像をダウンロードして試すことができます。\n","\n","トレーニングの精度が99%を超えているにもかかわらず、ネットワークが多くのミスをしていることに注意してください。\n","\n","これは**過学習**と呼ばれるものによるもので、ニューラルネットワークが非常に限られたデータで訓練されていることを意味します（各クラスの画像は500枚程度しかありません）。つまり、ニューラルネットワークはトレーニングデータと似ているデータについてはうまく分類することができますが、その中にないものの場合は失敗してしまうということです。\n","\n","これは、訓練するデータが多ければ多いほど、最終的なネットワークが良くなることを意味します。\n","\n","データが限られているにもかかわらず、トレーニングをより良いものにするために使用できるテクニックはたくさんあります。その中には画像拡張と呼ばれるものがあります。今回は範囲を超えているため説明を省略します。"]},{"cell_type":"code","metadata":{"id":"DoWp43WxJDNT","colab_type":"code","colab":{}},"source":["import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(300, 300))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a human\")\n","  else:\n","    print(fn + \" is a horse\")\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-8EHQyWGDvWz"},"source":["### Visualizing Intermediate Representations\n","\n","畳み込みネットワークが学習した特徴の状況を知るために、入力がどのように変換されていくかを視覚化してみましょう。\n","\n","学習セットからランダムな画像を選び、各レイヤーからのアウトプットを作成します。それぞれの行にある画像は、その特徴マップにある特定のフィルターです。このセルを再実行して、様々な訓練画像の中間表現を生成します。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-5tES8rXFjux","colab":{}},"source":["import numpy as np\n","import random\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","\n","# Let's define a new Model that will take an image as input, and will output\n","# intermediate representations for all layers in the previous model after\n","# the first.\n","successive_outputs = [layer.output for layer in model.layers[1:]]\n","#visualization_model = Model(img_input, successive_outputs)\n","visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n","# Let's prepare a random input image from the training set.\n","horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n","human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n","img_path = random.choice(horse_img_files + human_img_files)\n","\n","img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n","x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n","\n","# Rescale by 1/255\n","x /= 255\n","\n","# Let's run our image through our network, thus obtaining all\n","# intermediate representations for this image.\n","successive_feature_maps = visualization_model.predict(x)\n","\n","# These are the names of the layers, so can have them as part of our plot\n","layer_names = [layer.name for layer in model.layers]\n","\n","# Now let's display our representations\n","for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n","  if len(feature_map.shape) == 4:\n","    # Just do this for the conv / maxpool layers, not the fully-connected layers\n","    n_features = feature_map.shape[-1]  # number of features in feature map\n","    # The feature map has shape (1, size, size, n_features)\n","    size = feature_map.shape[1]\n","    # We will tile our images in this matrix\n","    display_grid = np.zeros((size, size * n_features))\n","    for i in range(n_features):\n","      # Postprocess the feature to make it visually palatable\n","      x = feature_map[0, :, :, i]\n","      x -= x.mean()\n","      x /= x.std()\n","      x *= 64\n","      x += 128\n","      x = np.clip(x, 0, 255).astype('uint8')\n","      # We'll tile each filter into this big horizontal grid\n","      display_grid[:, i * size : (i + 1) * size] = x\n","    # Display the grid\n","    scale = 20. / n_features\n","    plt.figure(figsize=(scale * n_features, scale))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tuqK2arJL0wo"},"source":["ご覧のように、画像の生のピクセルから、抽象的でコンパクトな表現へと変化していきます。出力層に近い表現は、その層のネットワークが何に注目しているかを表し、出力層に近づくにつれ注目している表現＝特徴量がどんどん簡単になり、注目すべき点が少なくなっていきます。これを「スパース」と呼びます。特徴量のスパース化は、ディープラーニングの重要な特徴です。\n","\n","これらの特徴量は、画像の元のピクセルに関する情報よりも、画像のクラス分けに関する情報と捉えることができ、学習をすすめることで洗練されていきます。そのため、畳み込みネットワーク（または一般的なディープネットワーク）は、情報蒸留パイプラインと考えることができます。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j4IBgYCYooGD"},"source":["## Clean Up\n","\n","次の演習を実行する前に、次のセルを実行してカーネルを終了させ、メモリリソースを解放します。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"651IgjLyo-Jx","colab":{}},"source":["import os, signal\n","os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":0,"outputs":[]}]}